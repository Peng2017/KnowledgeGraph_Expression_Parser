#!/usr/bin/env python3
import json
import asyncio
import argparse
from typing import Dict, Any, List, Optional, Union
from enum import Enum

class TokenType(Enum):
    """Token ç±»å‹æšä¸¾"""
    LITERAL = "LITERAL"      # æ•°å­—
    IDENTIFIER = "IDENTIFIER" # å˜é‡å
    OPERATOR = "OPERATOR"     # è¿ç®—ç¬¦
    LPAREN = "LPAREN"         # å·¦æ‹¬å·
    RPAREN = "RPAREN"         # å³æ‹¬å·
    EOF = "EOF"               # ç»“æŸç¬¦

class Token:
    """è¯æ³•å•å…ƒ"""
    def __init__(self, type_: TokenType, value: str, position: int = 0):
        self.type = type_
        self.value = value
        self.position = position
    
    def __repr__(self):
        return f"Token({self.type}, {self.value})"

class Lexer:
    """è¯æ³•åˆ†æå™¨"""
    def __init__(self, text: str):
        self.text = text.replace(' ', '')  # ç§»é™¤ç©ºæ ¼
        self.position = 0
        self.current_char = self.text[0] if text else None
    
    def advance(self):
        """ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªå­—ç¬¦"""
        self.position += 1
        if self.position >= len(self.text):
            self.current_char = None
        else:
            self.current_char = self.text[self.position]
    
    def read_number(self) -> str:
        """è¯»å–æ•°å­—"""
        result = ''
        while self.current_char is not None and (self.current_char.isdigit() or self.current_char == '.'):
            result += self.current_char
            self.advance()
        return result
    
    def read_identifier(self) -> str:
        """è¯»å–æ ‡è¯†ç¬¦"""
        result = ''
        while self.current_char is not None and (self.current_char.isalnum() or self.current_char == '_'):
            result += self.current_char
            self.advance()
        return result
    
    def tokenize(self) -> List[Token]:
        """è¯æ³•åˆ†æ"""
        tokens = []
        
        while self.current_char is not None:
            if self.current_char.isdigit():
                number = self.read_number()
                tokens.append(Token(TokenType.LITERAL, number, self.position))
            
            elif self.current_char.isalpha():
                identifier = self.read_identifier()
                tokens.append(Token(TokenType.IDENTIFIER, identifier, self.position))
            
            elif self.current_char == '(':
                tokens.append(Token(TokenType.LPAREN, '(', self.position))
                self.advance()
            
            elif self.current_char == ')':
                tokens.append(Token(TokenType.RPAREN, ')', self.position))
                self.advance()
            
            elif self.current_char in '+-*/':
                tokens.append(Token(TokenType.OPERATOR, self.current_char, self.position))
                self.advance()
            
            elif self.current_char == '^':
                tokens.append(Token(TokenType.OPERATOR, '^', self.position))
                self.advance()
            
            else:
                self.advance()
        
        tokens.append(Token(TokenType.EOF, '', self.position))
        return tokens

class ASTNode:
    """AST èŠ‚ç‚¹åŸºç±»"""
    pass

class NumberNode(ASTNode):
    """æ•°å­—èŠ‚ç‚¹"""
    def __init__(self, value: str):
        self.value = value
    
    def __repr__(self):
        return f"NumberNode({self.value})"

class VariableNode(ASTNode):
    """å˜é‡èŠ‚ç‚¹"""
    def __init__(self, name: str):
        self.name = name
    
    def __repr__(self):
        return f"VariableNode({self.name})"

class MultiOpNode(ASTNode):
    """å¤šå…ƒè¿ç®—èŠ‚ç‚¹ - æ”¯æŒåŒçº§è¿ç®—ç¬¦çš„å¤šå‰æ ‘ç»“æ„"""
    def __init__(self, operands: List[ASTNode], operators: List[str], priority: int):
        self.operands = operands  # æ“ä½œæ•°åˆ—è¡¨
        self.operators = operators  # æ“ä½œç¬¦åˆ—è¡¨
        self.priority = priority  # ä¼˜å…ˆçº§
    
    def __repr__(self):
        return f"MultiOpNode(operands={len(self.operands)}, ops={self.operators}, priority={self.priority})"

class ParenthesesNode(ASTNode):
    """æ‹¬å·èŠ‚ç‚¹ - å°†æ‹¬å·ä½œä¸ºç‹¬ç«‹çš„è¿ç®—ç¬¦å¤„ç†"""
    def __init__(self, inner_expr: ASTNode):
        self.inner_expr = inner_expr  # æ‹¬å·å†…çš„è¡¨è¾¾å¼
    
    def __repr__(self):
        return f"ParenthesesNode({self.inner_expr})"

class Parser:
    """é€’å½’ä¸‹é™è§£æå™¨ - æ”¹é€ ä¸ºå¤šå‰æ ‘ç»“æ„"""
    
    PRECEDENCE = {
        '+': 20, '-': 20,
        '*': 40, '/': 40,
        '^': 60
    }
    
    def __init__(self, tokens: List[Token]):
        self.tokens = tokens
        self.position = 0
        self.current_token = tokens[0] if tokens else Token(TokenType.EOF, '')
    
    def advance(self):
        """ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª Token"""
        self.position += 1
        if self.position < len(self.tokens):
            self.current_token = self.tokens[self.position]
        else:
            self.current_token = Token(TokenType.EOF, '')
    
    def get_precedence(self, operator: str) -> int:
        """è·å–è¿ç®—ç¬¦ä¼˜å…ˆçº§"""
        return self.PRECEDENCE.get(operator, -1)
    
    def parse_expression(self) -> ASTNode:
        """è§£æè¡¨è¾¾å¼ï¼ˆå…¥å£ç‚¹ï¼‰"""
        return self.parse_precedence_level(0)  # ä»æœ€ä½ä¼˜å…ˆçº§å¼€å§‹
    
    def parse_precedence_level(self, min_precedence: int) -> ASTNode:
        """æŒ‰ä¼˜å…ˆçº§å±‚æ¬¡è§£æï¼Œæ„å»ºå¤šå‰æ ‘"""
        left = self.parse_primary()
        
        while (self.current_token.type == TokenType.OPERATOR and 
               self.get_precedence(self.current_token.value) >= min_precedence):
            
            # æ”¶é›†ç›¸åŒä¼˜å…ˆçº§çš„æ“ä½œ
            operands = [left]
            operators = []
            current_precedence = self.get_precedence(self.current_token.value)
            
            # æ”¶é›†æ‰€æœ‰ç›¸åŒä¼˜å…ˆçº§çš„æ“ä½œ
            while (self.current_token.type == TokenType.OPERATOR and 
                   self.get_precedence(self.current_token.value) == current_precedence):
                
                op = self.current_token.value
                operators.append(op)
                self.advance()
                
                # è§£æå³æ“ä½œæ•°ï¼Œå¤„ç†æ›´é«˜ä¼˜å…ˆçº§
                right = self.parse_precedence_level(current_precedence + 1)
                operands.append(right)
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªæ“ä½œæ•°ï¼Œç›´æ¥è¿”å›
            if len(operands) == 1:
                left = operands[0]
            else:
                # åˆ›å»ºå¤šå‰æ ‘èŠ‚ç‚¹
                left = MultiOpNode(operands, operators, current_precedence)
        
        return left
    
    def parse_primary(self) -> ASTNode:
        """è§£æåŸºæœ¬è¡¨è¾¾å¼"""
        if self.current_token.type == TokenType.LITERAL:
            value = self.current_token.value
            self.advance()
            return NumberNode(value)
        
        elif self.current_token.type == TokenType.IDENTIFIER:
            name = self.current_token.value
            self.advance()
            return VariableNode(name)
        
        elif self.current_token.type == TokenType.LPAREN:
            self.advance()  # è·³è¿‡ '('
            expr = self.parse_expression()  # é€’å½’è§£ææ‹¬å·å†…å®¹
            if self.current_token.type != TokenType.RPAREN:
                raise SyntaxError(f"Expected ')' but got {self.current_token}")
            self.advance()  # è·³è¿‡ ')'
            # å°†æ‹¬å·ä½œä¸ºç‹¬ç«‹çš„è¿ç®—ç¬¦èŠ‚ç‚¹
            return ParenthesesNode(expr)
        
        elif self.current_token.type == TokenType.OPERATOR and self.current_token.value == '-':
            # è´Ÿå·å¤„ç†
            self.advance()
            operand = self.parse_primary()
            return MultiOpNode([NumberNode('0'), operand], ['-'], 20)
        
        else:
            raise SyntaxError(f"Unexpected token: {self.current_token}")

class KnowledgeGraphParser:
    """çŸ¥è¯†å›¾è°±è§£æå™¨ - å¤šå‰æ ‘ç»“æ„æ”¯æŒåŒçº§èŠ‚ç‚¹æ‰¹é‡å¤„ç†"""
    
    def __init__(self, debug_mode=False):
        self.debug_mode = debug_mode
        self.priority_levels = {'+': 1, '-': 1, '*': 2, '/': 2, '^': 3}
    
    def parse_to_kg(self, expression: str) -> Dict[str, Any]:
        """å°†è¡¨è¾¾å¼è§£æä¸ºå¤šå‰æ ‘çŸ¥è¯†å›¾è°±æ ¼å¼"""
        # è¯æ³•åˆ†æ
        lexer = Lexer(expression)
        tokens = lexer.tokenize()
        
        # è¯­æ³•åˆ†æ
        parser = Parser(tokens)
        ast_root = parser.parse_expression()
        
        # ä¿å­˜ AST ç»“æ„
        import os
        os.makedirs('.cache', exist_ok=True)
        self._save_ast_structure(ast_root, ".cache/output.ast", expression)
        
        # è½¬æ¢ä¸ºçŸ¥è¯†å›¾è°±
        kg_root = self._ast_to_kg(ast_root, [0], 0, expression)
        
        return {"root": kg_root}
    
    def _save_ast_structure(self, ast_root: ASTNode, filename: str, original_expr: str):
        """ä¿å­˜ AST ç»“æ„åˆ°æ–‡ä»¶"""
        content = []
        content.append("# AST è§£æç»“æœ (å¤šå‰æ ‘)\n")
        content.append(f"## åŸå§‹è¡¨è¾¾å¼: {original_expr}\n\n")
        
        content.append("## AST æ ‘å½¢ç»“æ„:\n```\n")
        self._format_ast_tree(ast_root, content, "")
        content.append("\n```\n\n")
        
        if self.debug_mode:
            content.append("## è°ƒè¯•ä¿¡æ¯:\n")
            content.append("### è§£æå™¨ç±»å‹: é€’å½’ä¸‹é™è§£æå™¨ï¼ˆå¤šå‰æ ‘ç»“æ„ï¼‰\n")
            content.append("### è¿ç®—ç¬¦ä¼˜å…ˆçº§è¡¨:\n")
            content.append("```\n")
            for op, prec in Parser.PRECEDENCE.items():
                content.append(f"{op}: {prec}\n")
            content.append("```\n")
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(''.join(content))
        
        if self.debug_mode:
            print(f"AST ç»“æ„å·²ä¿å­˜åˆ°: {filename}")
    
    def _format_ast_tree(self, node: ASTNode, lines: List[str], prefix: str):
        """æ ¼å¼åŒ– AST æ ‘å½¢ç»“æ„"""
        if isinstance(node, NumberNode):
            lines.append(f"NumberNode({node.value})\n")
        elif isinstance(node, VariableNode):
            lines.append(f"VariableNode({node.name})\n")
        elif isinstance(node, ParenthesesNode):
            lines.append(f"ParenthesesNode()\n")
            lines.append(f"{prefix}â””â”€â”€ inner: ")
            self._format_ast_tree(node.inner_expr, lines, prefix + "    ")
        elif isinstance(node, MultiOpNode):
            lines.append(f"MultiOpNode(priority={node.priority}, ops={node.operators})\n")
            for i, operand in enumerate(node.operands):
                is_last = i == len(node.operands) - 1
                branch = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
                next_prefix = "    " if is_last else "â”‚   "
                lines.append(f"{prefix}{branch}operand[{i}]: ")
                self._format_ast_tree(operand, lines, prefix + next_prefix)
    
    def _ast_to_kg(self, node: ASTNode, kg_id: List[int], depth: int, original_expr: str) -> Dict[str, Any]:
        """å°† AST èŠ‚ç‚¹è½¬æ¢ä¸ºçŸ¥è¯†å›¾è°±èŠ‚ç‚¹"""
        if isinstance(node, NumberNode):
            return {
                "KG_ID": kg_id,
                "expression": node.value,
                "is_variable": True,
                "operator": "",
                "depth": depth,
                "priority": 0
            }
        
        elif isinstance(node, VariableNode):
            return {
                "KG_ID": kg_id,
                "expression": node.name,
                "is_variable": True,
                "operator": "",
                "depth": depth,
                "priority": 0
            }
        
        elif isinstance(node, ParenthesesNode):
            # å¤„ç†æ‹¬å·èŠ‚ç‚¹
            child_id = kg_id + [0]
            child = self._ast_to_kg(node.inner_expr, child_id, depth + 1, original_expr)
            child["operator"] = "()"
            
            return {
                "KG_ID": kg_id,
                "expression": f"({self._extract_node_expression(node.inner_expr)})",
                "is_variable": False,
                "operator": "()",
                "depth": depth,
                "priority": 100,  # æ‹¬å·å…·æœ‰æœ€é«˜ä¼˜å…ˆçº§
                "children": [child]
            }
        
        elif isinstance(node, MultiOpNode):
            # æ„å»ºè¡¨è¾¾å¼å­—ç¬¦ä¸²
            if depth == 0:
                expression = original_expr
            else:
                expression = self._build_expression(node)
            
            # æ„å»ºå­èŠ‚ç‚¹ï¼ˆå¤šå‰æ ‘ï¼‰
            children = []
            for i, operand in enumerate(node.operands):
                child_id = kg_id + [i]
                child_op = "" if i == 0 else node.operators[i-1]
                child = self._ast_to_kg(operand, child_id, depth + 1, original_expr)
                child["operator"] = child_op
                children.append(child)
            
            return {
                "KG_ID": kg_id,
                "expression": expression,
                "is_variable": False,
                "operator": "",
                "depth": depth,
                "priority": self.priority_levels.get(node.operators[0] if node.operators else '', 0),
                "children": children
            }
    
    def _build_expression(self, node: MultiOpNode) -> str:
        """ä»å¤šå‰æ ‘èŠ‚ç‚¹æ„å»ºè¡¨è¾¾å¼å­—ç¬¦ä¸²"""
        if len(node.operands) == 1:
            return self._extract_node_expression(node.operands[0])
        
        parts = [self._extract_node_expression(node.operands[0])]
        for i, op in enumerate(node.operators):
            if i + 1 < len(node.operands):
                parts.append(op + self._extract_node_expression(node.operands[i + 1]))
        
        return ''.join(parts)
    
    def _extract_node_expression(self, node: ASTNode) -> str:
        """æå–èŠ‚ç‚¹çš„è¡¨è¾¾å¼å­—ç¬¦ä¸²"""
        if isinstance(node, NumberNode):
            return node.value
        elif isinstance(node, VariableNode):
            return node.name
        elif isinstance(node, ParenthesesNode):
            return f"({self._extract_node_expression(node.inner_expr)})"
        elif isinstance(node, MultiOpNode):
            return self._build_expression(node)
        return "unknown"
    
    # ä¿æŒä¸ V3.0 å…¼å®¹çš„æ–¹æ³•
    def get_leaf_nodes(self, kg_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰å¶å­èŠ‚ç‚¹"""
        leaves = []
        self._collect_leaves(kg_data["root"], leaves)
        return leaves
    
    def _collect_leaves(self, node: Dict[str, Any], leaves: List):
        """é€’å½’æ”¶é›†å¶å­èŠ‚ç‚¹"""
        if "children" not in node:
            leaves.append(node)
        else:
            for child in node["children"]:
                self._collect_leaves(child, leaves)
    
    def get_nodes_by_depth(self, kg_data: Dict[str, Any]) -> Dict[int, List[Dict]]:
        """æŒ‰æ·±åº¦åˆ†ç»„èŠ‚ç‚¹ - æ”¯æŒåŒçº§èŠ‚ç‚¹æ‰¹é‡å¤„ç†"""
        depth_groups = {}
        self._collect_by_depth(kg_data["root"], depth_groups)
        return depth_groups
    
    def _collect_by_depth(self, node: Dict[str, Any], groups: Dict):
        """æŒ‰æ·±åº¦æ”¶é›†èŠ‚ç‚¹"""
        depth = node["depth"]
        if depth not in groups:
            groups[depth] = []
        groups[depth].append(node)
        
        if "children" in node:
            for child in node["children"]:
                self._collect_by_depth(child, groups)
    
    # å¼‚æ­¥è®¡ç®—æ–¹æ³•ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
    async def get_variable_method(self, node: Dict[str, Any]) -> str:
        """è·å–å˜é‡å€¼çš„æ–¹æ³•"""
        await asyncio.sleep(0.01)
        return node["expression"]
    
    async def batch_process_same_level(self, nodes: List[Dict[str, Any]], process_type: str) -> List[str]:
        """æ‰¹é‡å¤„ç†åŒçº§èŠ‚ç‚¹ - æ ¸å¿ƒä¸šåŠ¡åŠŸèƒ½"""
        results = []
        
        if process_type == "llm_analysis":
            # LLMè¯­ä¹‰åˆ†æ
            for node in nodes:
                await asyncio.sleep(0.02)  # æ¨¡æ‹ŸLLMè°ƒç”¨
                result = f"LLMåˆ†æ({node['expression']})"
                results.append(result)
        
        elif process_type == "web_search":
            # ç½‘ç»œæœç´¢
            for node in nodes:
                await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿç½‘ç»œè¯·æ±‚
                result = f"æœç´¢ç»“æœ({node['expression']})"
                results.append(result)
        
        elif process_type == "financial_analysis":
            # è´¢åŠ¡æ•°æ®åˆ†æ
            for node in nodes:
                await asyncio.sleep(0.015)  # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
                result = f"è´¢åŠ¡æ•°æ®({node['expression']})"
                results.append(result)
        
        return results
    
    async def aggregate_from_leaves(self, kg_data: Dict[str, Any]) -> str:
        """ä»å¶å­èŠ‚ç‚¹å¼€å§‹å‘ä¸Šèšåˆ - å•å±‚è§¦å‘æœºåˆ¶"""
        result_cache = {}
        batch_counter = 0
        log_lines = []
        execution_flow = []  # æ–°å¢ï¼šè®°å½•æ‰§è¡Œæµç¨‹ç”¨äºå¯è§†åŒ–
        
        # æŒç»­æ£€æŸ¥å¹¶å¤„ç†å¯èšåˆçš„å±‚çº§ï¼Œç›´åˆ°å®Œæˆ
        while True:
            # æŸ¥æ‰¾å½“å‰å¯ä»¥èšåˆçš„èŠ‚ç‚¹ï¼ˆæ‰€æœ‰å…„å¼ŸèŠ‚ç‚¹éƒ½æ²¡æœ‰å­èŠ‚ç‚¹æˆ–å·²å®Œæˆï¼‰
            ready_nodes = self._find_ready_for_aggregation(kg_data, result_cache)
            
            if not ready_nodes:
                break  # æ²¡æœ‰æ›´å¤šå¯èšåˆçš„èŠ‚ç‚¹
            
            batch_counter += 1
            
            # è®°å½•å½“å‰æ‰¹æ¬¡å¹¶å‘å¤„ç†çš„ä»»åŠ¡èŠ‚ç‚¹
            current_batch_nodes = [f"[{','.join(map(str, node['KG_ID']))}]:{node['expression']}" for node in ready_nodes]
            
            # æ–°å¢ï¼šè®°å½•æ‰§è¡Œæµç¨‹ä¿¡æ¯
            batch_info = {
                'batch_id': batch_counter,
                'nodes': [],
                'operations': []
            }
            
            for node in ready_nodes:
                node_info = {
                    'id': ','.join(map(str, node['KG_ID'])),
                    'expression': node['expression'],
                    'is_variable': node['is_variable'],
                    'operator': node.get('operator', ''),
                    'operation_type': self._get_operation_type(node)
                }
                batch_info['nodes'].append(node_info)
            
            execution_flow.append(batch_info)
            
            # æ‰¹é‡å¤„ç†å½“å‰å±‚çš„èŠ‚ç‚¹
            await self.batch_process_same_level(ready_nodes, "llm_analysis")
            
            '''
            user: è¿™é‡Œçš„æ–¹æ³•ç­‰å€™å®¡æŸ¥ï¼Œä¼°è®¡éœ€è¦æ”¹è¿›ä¸ºå¯æ‰©å±•çš„æ–¹æ³•
            # æœªæ¥çš„åœºæ™¯å¯èƒ½æ˜¯ï¼š
            ## get_variable_method: å¯¹åº”ä¸Šç½‘æœç´¢æŸä¸ªç‰¹å®šå«ä¹‰çš„æ¦‚å¿µï¼Œæ¯”å¦‚ get_variable_method('PPOç®—æ³•')
            æ‰€ä»¥è¯¥æ­¥éª¤è¦æ±‚ï¼š
            1. node["is_variable"] çš„ç”Ÿæˆé€»è¾‘æœ€å¥½æ˜¯è¯¥èŠ‚ç‚¹ä¸å†å«æœ‰æ‹¬å·å’ŒåŠ å‡æ³•ä»¥å¤–çš„ operator
            ä¾‹å¦‚ï¼š
            èŠ‚ç‚¹ (A) æˆ– +A å¯ä»¥ä½œä¸ºget_variable_methodçš„è¾“å…¥ï¼ˆä»–ä»¬å¯¹åº”çš„operatoråˆ†åˆ«æ˜¯'()'å’Œ'+')
            get_variable_method('A+B') æˆ– get_variable_method('A*B')ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œæç¤ºè¾“å…¥å†…å®¹ä¸ºå¤æ‚è¡¨è¾¾å¼
            2. æœªæ¥A,A1,EXDFT,Bä¹‹ç±»çš„å˜é‡ï¼Œä¼šå¯¹åº”æ•°æ®åº“ä¸­çš„keyï¼Œget_variable_methodæ–¹æ³•ä½“å†…å†å®ç°æ ¹æ®è¾“å…¥çš„keyï¼Œå»æ•°æ®åº“è·å–çœŸæ­£row_dataçš„èƒ½åŠ›
            
            # æ‹¬å·
            å®šä¹‰ï¼šéœ€è¦æœ€ä¼˜å…ˆæ‰§è¡Œçš„ä»»åŠ¡
            æ¡ˆä¾‹ï¼š
            - æ¦‚å¿µè§£æï¼šæ¯”å¦‚ ("é‡‘åˆšçŸ³åŠå¯¼ä½“"ï¼‰ï¼Œæ­¤æ—¶å¤§è¯­è¨€æ¨¡å‹å¯èƒ½ä¸çŸ¥é“è¿™ä¸ªæ¦‚å¿µçš„ç¡®åˆ‡å«ä¹‰ï¼Œéœ€è¦äº¤ç»™æœç´¢å¼•æ“æˆ–å…¶ä»–ç½‘ç«™ï¼ˆè±†åŒ…ã€å…ƒå®ã€Geminiã€GPTã€Claudeï¼‰åˆæ­¥å›ç­”ï¼Œç•Œå®šæ¦‚å¿µèŒƒç•´

            # å¹‚è¿ç®—ï¼ˆä¼ä¸šå±‚é¢æ— æ³•æ§åˆ¶çš„å…¨è¡Œä¸šå½±å“å› ç´ ï¼‰
            å®šä¹‰ï¼šå®è§‚å¸‚åœºã€è¡Œä¸šèµ›é“
            æ¡ˆä¾‹ï¼š
            - è¡Œä¸šçˆ†å‘ï¼š AIæ¨ç†çš„äº•å–·ï¼Œå¸¦æ¥äº†æ‰€æœ‰å…³è”èµ›é“çš„æ”¶å…¥å‰§å¢
            - è¡Œä¸šæ»‘å¡ï¼š æˆ¿åœ°äº§è¡Œä¸šçªç„¶å´©ç›˜ã€æœªæ¥48ä¸ªæœˆAIåº”ç”¨èµ›é“æœ‰å´©ç›˜é£é™©
            - æ”¿ç­–å½±å“ï¼š ç¾å›½å¯¹ä¸­å›½AIä¼ä¸šDeepSeekå°æ€

            # ä¹˜é™¤æ³•ï¼ˆæœªæ¥ï¼šä¼ä¸šé€šè¿‡åŠªåŠ›å¯ä»¥æ”¹å˜çš„å¸‚åœºä»½é¢å’ŒæŠ€æœ¯é¢†å…ˆï¼‰
            å®šä¹‰ï¼šæ¥è‡ªç«äº‰ï¼ˆè“æµ·ï¼Œæˆ–åŠ å‰§ç«äº‰ï¼‰ã€å…¶ä»–æŠ€æœ¯è·¯çº¿çš„æŒ‘æˆ˜
            æ¡ˆä¾‹ï¼š
            - ä¹˜æ³•-å…¬å¸å¸‚åœºä»½é¢ä¼˜åŠ¿ï¼šOpenAIå¤„äºè¡Œä¸šå¤´éƒ¨åœ°ä½ã€è°·æ­Œæœç´¢ä¸šåŠ¡å å…¨çƒ90%ã€é«˜é€šSoCèŠ¯ç‰‡é¥é¥é¢†å…ˆã€Claudeæ­£åœ¨ç ”å‘æ–°ä¸€ä»£åŸºçŸ³æ¨¡å‹æå‡ç¼–ç¨‹èƒ½åŠ›
            - é™¤æ³•-å·¨å¤§æŒ‘æˆ˜ï¼šè°·æ­Œé¢ä¸´ä¸šåŠ¡æ‹†åˆ†ã€é«˜é€šå¤±å»è‹¹æœå®¢æˆ·ã€è‹±ä¼Ÿè¾¾æ¨ç†èŠ¯ç‰‡å°†é€æ­¥è¢«TPUå–ä»£
            æ³¨æ„ï¼šåœ¨æˆ‘ä»¬é¡¹ç›®ä¸­ï¼Œä¹˜é™¤æ³•å¤šæ¥è‡ªäºç½‘ç»œæœç´¢èŠ‚ç‚¹çš„æœç´ ç»“è®ºï¼ˆä¹˜æ³•è¡¨ç¤ºæ”¯æŒå‰ä¸€ä¸ªå˜é‡è§‚ç‚¹ï¼Œé™¤æ³•è¡¨ç¤ºæœç´¢ç»“è®ºä¸å‰ä¸€ä¸ªå˜é‡è§‚ç‚¹ç›¸æ‚–ï¼‰

            # åŠ å‡æ³•ï¼ˆç°åœ¨å’Œè¿‡å»çš„ä¼ä¸šäº‹å®ï¼‰
            å®šä¹‰ï¼šï¼ˆå¾€å¾€æ¥è‡ªæŠ•ç ”æŠ¥å‘Šç›®å½•æˆ–å…¶ä»–åœºæ™¯é€šç”¨æ¨¡æ¿ï¼‰å…¬å¸ä»‹ç»ã€ç®¡ç†å›¢é˜Ÿã€æ ¸å¿ƒäº§å“ã€æŠ€æœ¯ä¼˜åŠ¿ã€TAMã€è´¢åŠ¡è¡¨ç°ã€æŠ•èµ„é£é™©
            æ¡ˆä¾‹ï¼š
            - å‡æ³•-å…¬å¸è¿‡å»12ä¸ªæœˆæ”¶å…¥ä¸‹é™ã€Inteläº§å“è¢«å¸‚åœºæ·˜æ±°
            - åŠ æ³•-PEå†å²æœ€ä½ç‚¹ã€ARRåˆ›æ–°é«˜ã€ä¸ªåˆ«è®ºå›éƒ¨åˆ†ç½‘å‹çš„è´Ÿé¢æŠ¥å¯¼ï¼ˆç‰¹æ–¯æ‹‰æ’äººï¼‰
            æ³¨æ„ï¼šåœ¨æˆ‘ä»¬é¡¹ç›®ä¸­ï¼ŒåŠ å‡æ³•å¤šæ¥è‡ªäºå¤§è¯­è¨€æ¨¡å‹çš„çŒœæµ‹è§‚ç‚¹ï¼ˆLLMå¿…é¡»åœ¨æ¯è½®å¯¹å½¢æˆç»“è®ºçš„èŠ‚ç‚¹ç»™å‡º3ä¸ªæ­£é¢å’Œåé¢æ„è§ï¼‰

            # å˜é‡ï¼šåŸºæœ¬è§‚ç‚¹
            æ¯ä¸ªå˜é‡ï¼Œéƒ½éœ€è¦é€šè¿‡ æœç´¢-éªŒè¯ æ¥å®ç°ï¼ŒéªŒè¯æ˜¯éªŒè¯æƒå¨æ€§ï¼ˆä¸èƒ½æ¥è‡ªä¸å¯é çš„ç½‘ç«™ï¼Œé™¤éæ˜¯è´Ÿé¢è§‚ç‚¹ï¼‰

            *æ³¨æ„
            ä¹˜é™¤æ³•ä¸åŠ å‡æ³•å¯¹åº”çš„ä½ç½®ä¸æ˜¯ç‰¹åˆ«ä¸¥æ ¼ï¼Œæ¯”å¦‚ï¼š
            è°·æ­Œæœç´¢ä¸šåŠ¡å å…¨çƒ90% * è°·æ­ŒGeminiæ¨¡å‹å¸¦æ¥æ–°çš„AIæœç´¢äº§å“ => é¢„æµ‹æœç´¢ä¸šåŠ¡æ”¶å…¥è¿›ä¸€æ­¥ä¸Šå‡
            å¾€å¾€åè¿‡æ¥ä¹Ÿæˆç«‹ï¼ˆé¢„æµ‹ç»“æœçš„ä¸»è¯­æœ‰æ—¶ä¸å¤ªä¸€æ ·ï¼‰
            è°·æ­ŒGeminiæ¨¡å‹å¸¦æ¥æ–°çš„AIæœç´¢äº§å“ * è°·æ­Œæœç´¢ä¸šåŠ¡å å…¨çƒ90% => é¢„æµ‹AIä¸šåŠ¡æ”¶å…¥è¿›ä¸€æ­¥ä¸Šå‡
            
            '''
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡çš„æ¯ä¸ªèŠ‚ç‚¹
            parent_nodes = set()
            for node in ready_nodes:
                if node["is_variable"]:
                    result = await self.get_variable_method(node)
                    result_cache[str(node["KG_ID"])] = result
                    # è®°å½•çˆ¶èŠ‚ç‚¹IDï¼ˆå»æ‰æœ€åä¸€ä¸ªå…ƒç´ ï¼‰
                    if len(node["KG_ID"]) > 1:
                        parent_id = node["KG_ID"][:-1]
                        parent_nodes.add(str(parent_id))
                elif "children" in node:
                    result = await self._process_multi_node(node, result_cache)
                    result_cache[str(node["KG_ID"])] = result
                    # è®°å½•çˆ¶èŠ‚ç‚¹IDï¼ˆå»æ‰æœ€åä¸€ä¸ªå…ƒç´ ï¼‰
                    if len(node["KG_ID"]) > 1:
                        parent_id = node["KG_ID"][:-1]
                        parent_nodes.add(str(parent_id))
            
            # è®°å½•æœ¬æ‰¹æ¬¡æ—¥å¿—
            parent_ids_str = ', '.join(sorted(parent_nodes)) if parent_nodes else ""
            log_lines.append(f"æ‰¹æ¬¡{batch_counter}:")
            log_lines.append("[")
            for i, node in enumerate(current_batch_nodes):
                log_lines.append(f"  {node}")
            log_lines.append("]")
            log_lines.append("èšåˆåçš„çˆ¶èŠ‚ç‚¹ID:")
            log_lines.append(f"[{parent_ids_str}]")
            log_lines.append("")  # ç©ºè¡Œåˆ†éš”
        
        # è¾“å‡ºå¹¶å‘ä»»åŠ¡æ‰§è¡Œæ—¥å¿—
        import os
        os.makedirs('.cache', exist_ok=True)
        with open('.cache/output.log', 'w', encoding='utf-8') as f:
            f.write('# å¹¶å‘ä»»åŠ¡æ‰§è¡Œæ‰¹æ¬¡æ—¥å¿—\n')
            f.write('# æ ¼å¼ï¼šæ‰¹æ¬¡ä»»åŠ¡é¡ºåºç¼–å·, [å½“å‰æ‰¹æ¬¡å¹¶å‘å¤„ç†çš„ä»»åŠ¡èŠ‚ç‚¹], [èšåˆåçš„çˆ¶èŠ‚ç‚¹ID]\n\n')
            for line in log_lines:
                f.write(line + '\n')
        
        # æ–°å¢ï¼šç”Ÿæˆæ‰§è¡Œé¡ºåºå¯è§†åŒ–æ–‡ä»¶
        self._generate_execution_flow_visualization(execution_flow)
        
        if self.debug_mode:
            print(f"å¹¶å‘ä»»åŠ¡æ‰§è¡Œæ—¥å¿—å·²è¾“å‡ºåˆ°: .cache/output.log")
            print(f"æ‰§è¡Œé¡ºåºå¯è§†åŒ–å·²è¾“å‡ºåˆ°: .cache/execution_flow.mmd")
            print(f"æ‰§è¡Œé¡ºåºè¯¦ç»†ä¿¡æ¯å·²è¾“å‡ºåˆ°: .cache/execution_flow.json")
            print(f"æ–‡æœ¬æµç¨‹å›¾å·²è¾“å‡ºåˆ°: .cache/execution_flow.txt")
        
        root_id = str(kg_data["root"]["KG_ID"])
        return result_cache.get(root_id, "incomplete")
    
    def _find_ready_for_aggregation(self, kg_data: Dict[str, Any], result_cache: Dict[str, str]) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾å½“å‰å¯ä»¥èšåˆçš„èŠ‚ç‚¹ï¼ˆæ‰€æœ‰å…„å¼ŸèŠ‚ç‚¹éƒ½æ²¡æœ‰å­èŠ‚ç‚¹æˆ–å·²å®Œæˆï¼‰"""
        ready_nodes = []
        
        def traverse_nodes(nodes: List[Dict[str, Any]], parent_path: List[int] = []):
            for node in nodes:
                node_id = str(node["KG_ID"])
                
                # å¦‚æœèŠ‚ç‚¹å·²ç»å¤„ç†è¿‡ï¼Œè·³è¿‡
                if node_id in result_cache:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¶å­èŠ‚ç‚¹ï¼ˆå˜é‡èŠ‚ç‚¹ï¼‰
                if node["is_variable"]:
                    ready_nodes.append(node)
                    continue
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºå¯èšåˆçš„éå¶å­èŠ‚ç‚¹
                if "children" in node and self._children_completed(node, result_cache):
                    ready_nodes.append(node)
                    continue
                
                # é€’å½’å¤„ç†å­èŠ‚ç‚¹
                if "children" in node:
                    traverse_nodes(node["children"], node["KG_ID"])
        
        # å…ˆéå†æ‰€æœ‰èŠ‚ç‚¹
        if isinstance(kg_data, dict) and "root" in kg_data:
            root_node = kg_data["root"]
            
            # éå†æ ¹èŠ‚ç‚¹çš„å­èŠ‚ç‚¹
            if "children" in root_node:
                traverse_nodes(root_node["children"])
            
            # æœ€åæ£€æŸ¥æ ¹èŠ‚ç‚¹æœ¬èº«
            root_id = str(root_node["KG_ID"])
            if root_id not in result_cache and self._children_completed(root_node, result_cache):
                ready_nodes.append(root_node)
        elif isinstance(kg_data, dict) and "children" in kg_data:
            traverse_nodes(kg_data["children"])
        elif isinstance(kg_data, list):
            traverse_nodes(kg_data)
        
        return ready_nodes
    
    def _get_operation_type(self, node: Dict[str, Any]) -> str:
        """è·å–èŠ‚ç‚¹çš„æ“ä½œç±»å‹ï¼Œç”¨äºå¯è§†åŒ–"""
        if node["is_variable"]:
            return "å˜é‡è·å–"
        
        operator = node.get("operator", "")
        if operator == "()":
            return "æ‹¬å·è¿ç®—(æ¦‚å¿µè§£æ)"
        elif operator in ["+", "-"]:
            return "åŠ å‡æ³•(ä¼ä¸šäº‹å®)"
        elif operator in ["*", "/"]:
            return "ä¹˜é™¤æ³•(å¸‚åœºç«äº‰)"
        elif operator == "^":
            return "å¹‚è¿ç®—(è¡Œä¸šå› ç´ )"
        else:
            return "é»˜è®¤è¿ç®—"
    
    def _generate_execution_flow_visualization(self, execution_flow: List[Dict]) -> None:
        """ç”Ÿæˆæ‰§è¡Œé¡ºåºå¯è§†åŒ–æ–‡ä»¶"""
        import os
        import json
        
        # è¾“å‡ºè¯¦ç»†çš„æ‰§è¡Œæµç¨‹JSON
        with open('.cache/execution_flow.json', 'w', encoding='utf-8') as f:
            json.dump(execution_flow, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆMermaidæµç¨‹å›¾
        mermaid_lines = []
        mermaid_lines.append("graph TD")
        mermaid_lines.append("    %% çŸ¥è¯†å›¾è°±æ‰§è¡Œé¡ºåºå¯è§†åŒ–")
        mermaid_lines.append("")
        
        # æ·»åŠ å¼€å§‹èŠ‚ç‚¹
        mermaid_lines.append("    START([å¼€å§‹è§£æ])")
        
        prev_batch_nodes = ["START"]
        
        for batch in execution_flow:
            batch_id = batch['batch_id']
            nodes = batch['nodes']
            
            # ä¸ºæ¯ä¸ªæ‰¹æ¬¡åˆ›å»ºä¸€ä¸ªæ‰¹æ¬¡æ ‡è¯†èŠ‚ç‚¹
            batch_node = f"BATCH{batch_id}"
            mermaid_lines.append(f"    {batch_node}[æ‰¹æ¬¡{batch_id}]")
            
            # è¿æ¥ä¸Šä¸€æ‰¹æ¬¡åˆ°å½“å‰æ‰¹æ¬¡
            for prev_node in prev_batch_nodes:
                mermaid_lines.append(f"    {prev_node} --> {batch_node}")
            
            # ä¸ºå½“å‰æ‰¹æ¬¡çš„æ¯ä¸ªèŠ‚ç‚¹åˆ›å»ºèŠ‚ç‚¹
            current_batch_nodes = []
            for i, node in enumerate(nodes):
                node_id = f"N{batch_id}_{i}"
                node_label = f"{node['expression']}\\n({node['operation_type']})"
                
                # æ ¹æ®æ“ä½œç±»å‹è®¾ç½®ä¸åŒçš„æ ·å¼
                if node['is_variable']:
                    mermaid_lines.append(f"    {node_id}[{node_label}]")
                    mermaid_lines.append(f"    {node_id} --> {node_id}_result{{è·å–: {node['expression']}}}")
                    current_batch_nodes.append(f"{node_id}_result")
                else:
                    mermaid_lines.append(f"    {node_id}[{node_label}]")
                    mermaid_lines.append(f"    {node_id} --> {node_id}_result{{èšåˆ: {node['expression']}}}")
                    current_batch_nodes.append(f"{node_id}_result")
                
                # è¿æ¥æ‰¹æ¬¡èŠ‚ç‚¹åˆ°å…·ä½“æ“ä½œèŠ‚ç‚¹
                mermaid_lines.append(f"    {batch_node} --> {node_id}")
            
            prev_batch_nodes = current_batch_nodes
        
        # æ·»åŠ ç»“æŸèŠ‚ç‚¹
        mermaid_lines.append("    END([å®Œæˆèšåˆ])")
        for node in prev_batch_nodes:
            mermaid_lines.append(f"    {node} --> END")
        
        # æ·»åŠ æ ·å¼å®šä¹‰
        mermaid_lines.append("")
        mermaid_lines.append("    %% æ ·å¼å®šä¹‰")
        mermaid_lines.append("    classDef batchClass fill:#e1f5fe,stroke:#01579b,stroke-width:2px")
        mermaid_lines.append("    classDef variableClass fill:#f3e5f5,stroke:#4a148c,stroke-width:2px")
        mermaid_lines.append("    classDef operationClass fill:#fff3e0,stroke:#e65100,stroke-width:2px")
        mermaid_lines.append("    classDef resultClass fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px")
        
        # è¾“å‡ºMermaidæ–‡ä»¶
        with open('.cache/execution_flow.mmd', 'w', encoding='utf-8') as f:
            f.write('\n'.join(mermaid_lines))
        
        # ç”Ÿæˆç®€åŒ–çš„æ–‡æœ¬æµç¨‹å›¾
        self._generate_simple_text_flow(execution_flow)
    
    def _generate_simple_text_flow(self, execution_flow: List[Dict]) -> None:
        """ç”Ÿæˆç®€åŒ–çš„æ–‡æœ¬æµç¨‹å›¾ï¼Œæ›´æ˜“è¯»çš„æ ¼å¼"""
        text_lines = []
        text_lines.append("# çŸ¥è¯†å›¾è°±æ‰§è¡Œé¡ºåºæµç¨‹å›¾")
        text_lines.append("")
        text_lines.append("```")
        text_lines.append("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        text_lines.append("â”‚   å¼€å§‹è§£æ      â”‚")
        text_lines.append("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        text_lines.append("         â”‚")
        text_lines.append("         â–¼")
        
        for i, batch in enumerate(execution_flow):
            batch_id = batch['batch_id']
            nodes = batch['nodes']
            
            # æ‰¹æ¬¡æ ‡é¢˜
            text_lines.append(f"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
            text_lines.append(f"â”‚   æ‰¹æ¬¡ {batch_id}        â”‚")
            text_lines.append(f"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            
            # å¹¶å‘æ‰§è¡Œçš„èŠ‚ç‚¹
            if len(nodes) > 1:
                text_lines.append("         â”‚")
                text_lines.append("    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”")
                
                # ç»˜åˆ¶å¹¶å‘åˆ†æ”¯
                for j, node in enumerate(nodes):
                    if j == 0:
                        text_lines.append(f"    â–¼         â–¼")
                    
                    operation_symbol = self._get_operation_symbol(node)
                    expr_short = node['expression'][:8] + '...' if len(node['expression']) > 8 else node['expression']
                    
                    if j % 2 == 0:  # å·¦ä¾§
                        text_lines.append(f"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
                        text_lines.append(f"â”‚{operation_symbol} {expr_short:<7}â”‚ â”‚         â”‚")
                        text_lines.append(f"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
                    else:  # å³ä¾§
                        # ä¿®æ”¹ä¸Šä¸€è¡Œ
                        text_lines[-2] = text_lines[-2].replace("â”‚         â”‚", f"â”‚{operation_symbol} {expr_short:<7}â”‚")
                
                # åˆå¹¶å›ä¸»æµç¨‹
                text_lines.append("    â”‚         â”‚")
                text_lines.append("    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜")
                text_lines.append("         â”‚")
            else:
                # å•ä¸ªèŠ‚ç‚¹
                node = nodes[0]
                operation_symbol = self._get_operation_symbol(node)
                expr_short = node['expression'][:10] + '...' if len(node['expression']) > 10 else node['expression']
                
                text_lines.append("         â”‚")
                text_lines.append("         â–¼")
                text_lines.append(f"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
                text_lines.append(f"â”‚{operation_symbol} {expr_short:<13}â”‚")
                text_lines.append(f"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªæ‰¹æ¬¡ï¼Œæ·»åŠ è¿æ¥çº¿
            if i < len(execution_flow) - 1:
                text_lines.append("         â”‚")
                text_lines.append("         â–¼")
        
        # ç»“æŸ
        text_lines.append("         â”‚")
        text_lines.append("         â–¼")
        text_lines.append("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        text_lines.append("â”‚   å®Œæˆèšåˆ      â”‚")
        text_lines.append("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        text_lines.append("```")
        
        # æ·»åŠ å›¾ä¾‹è¯´æ˜
        text_lines.append("")
        text_lines.append("## å›¾ä¾‹è¯´æ˜")
        text_lines.append("")
        text_lines.append("- ğŸ”¤ å˜é‡è·å–ï¼šä»æ•°æ®åº“æˆ–æœç´¢å¼•æ“è·å–åŸºç¡€æ•°æ®")
        text_lines.append("- ğŸ“¦ æ‹¬å·è¿ç®—ï¼šæ¦‚å¿µè§£æï¼Œéœ€è¦LLMç†è§£å¤æ‚æ¦‚å¿µ")
        text_lines.append("- â• åŠ å‡æ³•ï¼šä¼ä¸šå†å²äº‹å®å’Œç°çŠ¶åˆ†æ")
        text_lines.append("- âœ–ï¸ ä¹˜é™¤æ³•ï¼šå¸‚åœºç«äº‰å’ŒæŠ€æœ¯é¢†å…ˆæ€§åˆ†æ")
        text_lines.append("- ğŸ”º å¹‚è¿ç®—ï¼šè¡Œä¸šå®è§‚å› ç´ å’Œæ”¿ç­–å½±å“")
        text_lines.append("")
        
        # æ·»åŠ è¯¦ç»†çš„æ‰¹æ¬¡ä¿¡æ¯
        text_lines.append("## è¯¦ç»†æ‰§è¡Œä¿¡æ¯")
        text_lines.append("")
        for batch in execution_flow:
            text_lines.append(f"### æ‰¹æ¬¡ {batch['batch_id']}")
            text_lines.append("")
            for node in batch['nodes']:
                text_lines.append(f"- **{node['expression']}** ({node['operation_type']})")
                if node['operator']:
                    text_lines.append(f"  - æ“ä½œç¬¦: `{node['operator']}`")
                text_lines.append(f"  - èŠ‚ç‚¹ID: `{node['id']}`")
                text_lines.append("")
        
        # è¾“å‡ºæ–‡æœ¬æµç¨‹å›¾
        with open('.cache/execution_flow.txt', 'w', encoding='utf-8') as f:
            f.write('\n'.join(text_lines))
    
    def _get_operation_symbol(self, node: Dict[str, Any]) -> str:
        """è·å–æ“ä½œçš„ç¬¦å·è¡¨ç¤º"""
        if node['is_variable']:
            return "ğŸ”¤"
        
        operator = node.get('operator', '')
        if operator == "()":
            return "ğŸ“¦"
        elif operator in ["+", "-"]:
            return "â•"
        elif operator in ["*", "/"]:
            return "âœ–ï¸"
        elif operator == "^":
            return "ğŸ”º"
        else:
            return "âš™ï¸"
    
    def _children_completed(self, node: Dict[str, Any], result_cache: Dict[str, str]) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹æ˜¯å¦éƒ½å·²å®Œæˆè®¡ç®—"""
        if "children" not in node:
            return True
        
        for child in node["children"]:
            child_id = str(child["KG_ID"])
            if child_id not in result_cache:
                return False
        return True
    
    async def _process_multi_node(self, node: Dict[str, Any], cache: Dict):
        """å¤„ç†å¤šå‰æ ‘èŠ‚ç‚¹ - è°ƒåº¦åˆ°å…·ä½“çš„åŸå­æ“ä½œæ–¹æ³•"""
        children = node["children"]
        
        if len(children) == 1:
            # å•å­èŠ‚ç‚¹å¤„ç†
            child_result = cache[str(children[0]['KG_ID'])]
            if children[0].get("operator") == "()":
                return await self._process_parentheses_operation(node, cache)
            return child_result
        
        # å¤šå­èŠ‚ç‚¹ï¼šæ ¹æ®è¿ç®—ç¬¦ç±»å‹è°ƒåº¦åˆ°å¯¹åº”çš„åŸå­æ“ä½œæ–¹æ³•
        operators = [children[i]["operator"] for i in range(1, len(children))]
        
        # æ£€æŸ¥è¿ç®—ç¬¦ç±»å‹å¹¶è°ƒåº¦
        if any(op in ['+', '-'] for op in operators):
            return await self._process_addition_subtraction_operation(node, cache)
        elif any(op in ['*', '/'] for op in operators):
            return await self._process_multiplication_division_operation(node, cache)
        elif any(op == '^' for op in operators):
            return await self._process_power_operation(node, cache)
        else:
            # é»˜è®¤å¤„ç†
            return await self._process_default_operation(node, cache)
    
    async def _process_parentheses_operation(self, node: Dict[str, Any], cache: Dict) -> str:
        """æ‹¬å·è¿ç®—åŸå­æ“ä½œ - æœ€ä¼˜å…ˆæ‰§è¡Œçš„ä»»åŠ¡
        
        å®šä¹‰ï¼šéœ€è¦æœ€ä¼˜å…ˆæ‰§è¡Œçš„ä»»åŠ¡
        æ¡ˆä¾‹ï¼šæ¦‚å¿µè§£æï¼Œæ¯”å¦‚ ("é‡‘åˆšçŸ³åŠå¯¼ä½“")ï¼Œéœ€è¦äº¤ç»™æœç´¢å¼•æ“æˆ–LLMåˆæ­¥å›ç­”
        
        å½“å‰ç‰ˆæœ¬ï¼šç®€å•å­—ç¬¦ä¸²åŒ…è£…
        æœªæ¥æ‰©å±•ï¼šå¯æ·»åŠ æ¦‚å¿µè§£æã€æœç´¢å¼•æ“æŸ¥è¯¢ç­‰åŠŸèƒ½
        """
        print("[DEBUG] æ‰§è¡Œæ‹¬å·è¿ç®—æ–¹æ³•: _process_parentheses_operation")
        children = node["children"]
        child_result = cache[str(children[0]['KG_ID'])]
        
        # å½“å‰ç‰ˆæœ¬ï¼šç®€å•å­—ç¬¦ä¸²ç›¸åŠ 
        result = f"({child_result})"
        
        # TODO: æœªæ¥æ‰©å±•ç‚¹
        # - æ¦‚å¿µè§£æï¼šè°ƒç”¨æœç´¢å¼•æ“æˆ–LLMè§£é‡Šæ¦‚å¿µ
        # - æƒå¨æ€§éªŒè¯ï¼šæ£€æŸ¥ä¿¡æ¯æ¥æºå¯é æ€§
        
        return result
    
    async def _process_addition_subtraction_operation(self, node: Dict[str, Any], cache: Dict) -> str:
        """åŠ å‡æ³•è¿ç®—åŸå­æ“ä½œ - ç°åœ¨å’Œè¿‡å»çš„ä¼ä¸šäº‹å®
        
        å®šä¹‰ï¼šå…¬å¸ä»‹ç»ã€ç®¡ç†å›¢é˜Ÿã€æ ¸å¿ƒäº§å“ã€æŠ€æœ¯ä¼˜åŠ¿ã€TAMã€è´¢åŠ¡è¡¨ç°ã€æŠ•èµ„é£é™©
        æ¡ˆä¾‹ï¼š
        - åŠ æ³•ï¼šPEå†å²æœ€ä½ç‚¹ã€ARRåˆ›æ–°é«˜
        - å‡æ³•ï¼šå…¬å¸è¿‡å»12ä¸ªæœˆæ”¶å…¥ä¸‹é™ã€Inteläº§å“è¢«å¸‚åœºæ·˜æ±°
        
        å½“å‰ç‰ˆæœ¬ï¼šç®€å•å­—ç¬¦ä¸²ç›¸åŠ 
        æœªæ¥æ‰©å±•ï¼šLLMå¿…é¡»ç»™å‡º3ä¸ªæ­£é¢å’Œåé¢æ„è§
        """
        print("[DEBUG] æ‰§è¡ŒåŠ å‡æ³•è¿ç®—æ–¹æ³•: _process_addition_subtraction_operation")
        children = node["children"]
        result = cache[str(children[0]["KG_ID"])]
        
        for i in range(1, len(children)):
            child_val = cache[str(children[i]["KG_ID"])]
            operator = children[i]["operator"]
            
            # å½“å‰ç‰ˆæœ¬ï¼šç®€å•å­—ç¬¦ä¸²ç›¸åŠ 
            result = f"{result}{operator}{child_val}"
            
            # TODO: æœªæ¥æ‰©å±•ç‚¹
            # - åŠ æ³•ï¼šLLMåˆ†ææ­£é¢å› ç´ ï¼Œç”Ÿæˆæ”¯æŒè§‚ç‚¹
            # - å‡æ³•ï¼šLLMåˆ†æè´Ÿé¢å› ç´ ï¼Œç”Ÿæˆé£é™©è¯„ä¼°
            # - æƒå¨æ€§éªŒè¯ï¼šç¡®ä¿ä¿¡æ¯æ¥æºå¯é 
        
        return result
    
    async def _process_multiplication_division_operation(self, node: Dict[str, Any], cache: Dict) -> str:
        """ä¹˜é™¤æ³•è¿ç®—åŸå­æ“ä½œ - ä¼ä¸šé€šè¿‡åŠªåŠ›å¯ä»¥æ”¹å˜çš„å¸‚åœºä»½é¢å’ŒæŠ€æœ¯é¢†å…ˆ
        
        å®šä¹‰ï¼šæ¥è‡ªç«äº‰ï¼ˆè“æµ·ï¼Œæˆ–åŠ å‰§ç«äº‰ï¼‰ã€å…¶ä»–æŠ€æœ¯è·¯çº¿çš„æŒ‘æˆ˜
        æ¡ˆä¾‹ï¼š
        - ä¹˜æ³•ï¼šOpenAIå¤„äºè¡Œä¸šå¤´éƒ¨åœ°ä½ã€è°·æ­Œæœç´¢ä¸šåŠ¡å å…¨çƒ90%
        - é™¤æ³•ï¼šè°·æ­Œé¢ä¸´ä¸šåŠ¡æ‹†åˆ†ã€é«˜é€šå¤±å»è‹¹æœå®¢æˆ·
        
        å½“å‰ç‰ˆæœ¬ï¼šç®€å•å­—ç¬¦ä¸²ç›¸åŠ 
        æœªæ¥æ‰©å±•ï¼šç½‘ç»œæœç´¢èŠ‚ç‚¹çš„æœç´¢ç»“è®ºï¼ˆä¹˜æ³•è¡¨ç¤ºæ”¯æŒï¼Œé™¤æ³•è¡¨ç¤ºç›¸æ‚–ï¼‰
        """
        print("[DEBUG] æ‰§è¡Œä¹˜é™¤æ³•è¿ç®—æ–¹æ³•: _process_multiplication_division_operation")
        children = node["children"]
        result = cache[str(children[0]["KG_ID"])]
        
        for i in range(1, len(children)):
            child_val = cache[str(children[i]["KG_ID"])]
            operator = children[i]["operator"]
            
            # å½“å‰ç‰ˆæœ¬ï¼šç®€å•å­—ç¬¦ä¸²ç›¸åŠ 
            result = f"{result}{operator}{child_val}"
            
            # TODO: æœªæ¥æ‰©å±•ç‚¹
            # - ä¹˜æ³•ï¼šç½‘ç»œæœç´¢æ”¯æŒå‰ä¸€ä¸ªå˜é‡è§‚ç‚¹çš„è¯æ®
            # - é™¤æ³•ï¼šç½‘ç»œæœç´¢ä¸å‰ä¸€ä¸ªå˜é‡è§‚ç‚¹ç›¸æ‚–çš„è¯æ®
            # - ç«äº‰åˆ†æï¼šå¸‚åœºä»½é¢ã€æŠ€æœ¯é¢†å…ˆæ€§è¯„ä¼°
        
        return result
    
    async def _process_power_operation(self, node: Dict[str, Any], cache: Dict) -> str:
        """å¹‚è¿ç®—åŸå­æ“ä½œ - ä¼ä¸šå±‚é¢æ— æ³•æ§åˆ¶çš„å…¨è¡Œä¸šå½±å“å› ç´ 
        
        å®šä¹‰ï¼šå®è§‚å¸‚åœºã€è¡Œä¸šèµ›é“
        æ¡ˆä¾‹ï¼š
        - è¡Œä¸šçˆ†å‘ï¼šAIæ¨ç†çš„äº•å–·ï¼Œå¸¦æ¥æ‰€æœ‰å…³è”èµ›é“çš„æ”¶å…¥å‰§å¢
        - è¡Œä¸šæ»‘å¡ï¼šæˆ¿åœ°äº§è¡Œä¸šçªç„¶å´©ç›˜
        - æ”¿ç­–å½±å“ï¼šç¾å›½å¯¹ä¸­å›½AIä¼ä¸šDeepSeekå°æ€
        
        å½“å‰ç‰ˆæœ¬ï¼šç®€å•å­—ç¬¦ä¸²ç›¸åŠ 
        æœªæ¥æ‰©å±•ï¼šå®è§‚ç»æµã€æ”¿ç­–å½±å“ã€è¡Œä¸šè¶‹åŠ¿åˆ†æ
        """
        print("[DEBUG] æ‰§è¡Œå¹‚è¿ç®—æ–¹æ³•: _process_power_operation")
        children = node["children"]
        result = cache[str(children[0]["KG_ID"])]
        
        for i in range(1, len(children)):
            child_val = cache[str(children[i]["KG_ID"])]
            operator = children[i]["operator"]
            
            # å½“å‰ç‰ˆæœ¬ï¼šç®€å•å­—ç¬¦ä¸²ç›¸åŠ 
            result = f"{result}{operator}{child_val}"
            
            # TODO: æœªæ¥æ‰©å±•ç‚¹
            # - å®è§‚ç»æµåˆ†æï¼šè¡Œä¸šæ•´ä½“è¶‹åŠ¿è¯„ä¼°
            # - æ”¿ç­–å½±å“åˆ†æï¼šæ”¿åºœæ”¿ç­–å¯¹è¡Œä¸šçš„å½±å“
            # - ä¸å¯æ§å› ç´ ï¼šä¼ä¸šæ— æ³•æ”¹å˜çš„å¤–éƒ¨ç¯å¢ƒ
        
        return result
    
    async def _process_default_operation(self, node: Dict[str, Any], cache: Dict) -> str:
        """é»˜è®¤è¿ç®—åŸå­æ“ä½œ - å¤„ç†å…¶ä»–æœªåˆ†ç±»çš„è¿ç®—ç¬¦
        
        å½“å‰ç‰ˆæœ¬ï¼šç®€å•å­—ç¬¦ä¸²ç›¸åŠ 
        æœªæ¥æ‰©å±•ï¼šæ ¹æ®å…·ä½“ä¸šåŠ¡éœ€æ±‚æ·»åŠ æ–°çš„è¿ç®—ç¬¦å¤„ç†é€»è¾‘
        """
        print("[DEBUG] æ‰§è¡Œé»˜è®¤è¿ç®—æ–¹æ³•: _process_default_operation")
        children = node["children"]
        result = cache[str(children[0]["KG_ID"])]
        
        for i in range(1, len(children)):
            child_val = cache[str(children[i]["KG_ID"])]
            operator = children[i]["operator"]
            
            # å½“å‰ç‰ˆæœ¬ï¼šç®€å•å­—ç¬¦ä¸²ç›¸åŠ 
            if operator == "()":
                result = f"({child_val})"
            else:
                result = f"{result}{operator}{child_val}"
        
        return result
    
    def to_markdown_tree(self, kg_data: Dict[str, Any]) -> str:
        """å°†çŸ¥è¯†å›¾è°±è½¬æ¢ä¸º Markdown æ ‘ç»“æ„"""
        lines = ["# çŸ¥è¯†å›¾è°±ç»“æ„ç¤ºä¾‹ (å¤šå‰æ ‘)\n\n```json\n"]
        self._add_markdown_children(kg_data["root"], lines, "", True)
        lines.append("```\n")
        return ''.join(lines)
    
    def _add_markdown_children(self, node: Dict[str, Any], lines: List[str], prefix: str, is_last: bool = True):
        """æ·»åŠ  Markdown å­èŠ‚ç‚¹"""
        kg_id = node["KG_ID"]
        expr = node["expression"]
        op = node["operator"]
        is_var = node["is_variable"]
        depth = node["depth"]
        priority = node["priority"]
        
        # æ ¼å¼åŒ–èŠ‚ç‚¹ID
        id_str = "[" + ",".join(map(str, kg_id)) + "]"
        
        # æ„å»ºå®Œæ•´çš„JSONå­—æ®µä¿¡æ¯
        json_info = f'{{"KG_ID": {kg_id}, "expression": "{expr}", "is_variable": {str(is_var).lower()}, "operator": "{op}", "depth": {depth}, "priority": {priority}}}'
        
        # æ ¹èŠ‚ç‚¹ä¸éœ€è¦åˆ†æ”¯ç¬¦å·
        if prefix == "":
            lines.append(f"{id_str}: {json_info}\n")
        else:
            branch = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            lines.append(f"{prefix}{branch}{id_str}: {json_info}\n")
        
        if "children" in node and node["children"]:
            child_prefix = prefix + ("    " if is_last else "â”‚   ")
            for i, child in enumerate(node["children"]):
                is_child_last = i == len(node["children"]) - 1
                self._add_markdown_children(child, lines, child_prefix, is_child_last)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='çŸ¥è¯†å›¾è°±è§£æå™¨ - å¤šå‰æ ‘ç»“æ„')
    parser.add_argument('expression', nargs='?', default='A1^2+((B1*2-C1)/D1^(E+F)+G)*H-1', 
                       help='è¦è§£æçš„æ•°å­¦è¡¨è¾¾å¼')
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    print(f"ä½¿ç”¨è¡¨è¾¾å¼: {args.expression}")
    
    # åˆ›å»ºè§£æå™¨
    kg_parser = KnowledgeGraphParser(debug_mode=args.debug)
    
    # è§£æä¸ºçŸ¥è¯†å›¾è°±
    kg_data = kg_parser.parse_to_kg(args.expression)
    
    # è¾“å‡ºåˆ°æ–‡ä»¶
    import os
    os.makedirs('.cache', exist_ok=True)
    with open('.cache/output.json', 'w', encoding='utf-8') as f:
        json.dump(kg_data, f, ensure_ascii=False, indent=2)
    print("çŸ¥è¯†å›¾è°±å·²è¾“å‡ºåˆ°: .cache/output.json")
    
    # è¾“å‡º Markdown æ ‘ç»“æ„
    markdown_tree = kg_parser.to_markdown_tree(kg_data)
    with open('.cache/output.md', 'w', encoding='utf-8') as f:
        f.write(markdown_tree)
    print("Markdown æ ‘ç»“æ„å·²è¾“å‡ºåˆ°: .cache/output.md")
    
    print("\n=== æ‰§è¡Œé¡ºåºå¯è§†åŒ–æ–‡ä»¶ ===")
    print("Mermaidæµç¨‹å›¾: .cache/execution_flow.mmd")
    print("æ–‡æœ¬æµç¨‹å›¾: .cache/execution_flow.txt")
    print("è¯¦ç»†æ‰§è¡Œä¿¡æ¯: .cache/execution_flow.json")
    
    # æ¼”ç¤ºåŒçº§èŠ‚ç‚¹æ‰¹é‡å¤„ç†
    if args.debug:
        print("\n=== åŒçº§èŠ‚ç‚¹æ‰¹é‡å¤„ç†æ¼”ç¤º ===")
        depth_groups = kg_parser.get_nodes_by_depth(kg_data)
        for depth, nodes in depth_groups.items():
            if depth > 0 and len(nodes) > 1:  # æœ‰å¤šä¸ªåŒçº§èŠ‚ç‚¹
                print(f"\næ·±åº¦ {depth} çš„åŒçº§èŠ‚ç‚¹æ•°é‡: {len(nodes)}")
                for node in nodes:
                    print(f"  - {node['expression']} (ä¼˜å…ˆçº§: {node['priority']})")
    
    # å¼‚æ­¥èšåˆè®¡ç®—
    async def run_aggregation():
        result = await kg_parser.aggregate_from_leaves(kg_data)
        print(f"èšåˆç»“æœ: {result}")
    
    asyncio.run(run_aggregation())

if __name__ == "__main__":
    main()